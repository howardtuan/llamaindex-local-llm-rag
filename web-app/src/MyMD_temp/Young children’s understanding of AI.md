Education and Information Technologies
https://doi.org/10.1007/s10639-024-13169-x

Young children’s understanding of AI

Dagmar Mercedes Heeg1

 · Lucy Avraamidou1

Received: 7 June 2024 / Accepted: 7 November 2024
© The Author(s) 2024

Abstract
AI has become integral to daily life. Teaching, learning, and research are no excep-
tion. However, most studies on education have approached AI as a technology and
focused mostly on learning outcomes rather than understanding student engagement
and sense-making of AI as a socio-cultural tool with impact on their daily lives. To
address  this  gap  in  the  knowledge  base,  we  performed  a  qualitative  case  study  to
explore young children’s conceptualization of AI not only as a technology but also
as a tool utilised in their everyday lives. We collected data through semi-structured
group  interviews  with  eighteen  children  aged  11  to  12  and  thematically  analyzed
the data through a combination of deductive and inductive coding techniques. The
findings  suggest  that:  a)  children’s  conceptualizations  of  AI  as  a  technology  are
grounded in their personal experiences; b) children have a socio-cultural approach
to AI in which they experience and understand AI as first and foremost a supportive
tool; and, c) children exhibit a high level of engagement with ethics of AI, showing
a keen interest in the socio-cultural implications, particularly about AI applications
with which they are familiar. Based on these findings and grounded within existing
literature, we offer a set of recommendations for the design of engaging and person-
ally relevant AI education curriculum materials for young children with critical AI
literacy at the forefront.

Keywords  AI education · AI ethics · AI literacy · Young children

1  Introduction

With  the  rapid  expansion  of  Artificial  intelligence  (AI)  in  industry,  education,
and  everyday  life,  children  are  living  in  and  facing  an  increasingly  AI-powered
society.  In  the  last  decade,  researchers  have  explored  AI’s  potential  to  address

 *  Dagmar Mercedes Heeg
d.m.heeg@rug.nl

1  University of Groningen, Nijenborgh 7, Groningen 9747AG, Kingdom of the Netherlands

Vol.:(0123456789)
Education and Information Technologies

educational challenges (e.g. achievement gaps, unequity, high workload) through
tools such as intelligent tutoring systems, chatbots, or automated feedback mod-
els  (Heeg  &  Avraamidou,  2023).  More  recently,  research  efforts  have  shifted
from  teaching  ‘with’  AI,  to  teaching  ‘about’  AI,  as  contemporary  reform  docu-
ments have widely called for ‘AI literacy’ as a new set of skills and competencies
that  future  student  generations  need  to  responsibly  and  ethically  function  in  an
AI-powered society (OECD, 2021; UNESCO, 2022).

As a response, all around the world, attention is placed on developing AI educa-
tion for schools (e.g. Casal-Otero et al., 2023; Su et al., 2023a, b; Yue et al., 2022).
In China, AI education is part of the mandatory high school curriculum and a series
of seven AI textbooks is already available for elementary, middle, and high schools
(Chen & Tang, 2018; Xiong et al., 2018). In the rest of the world, no national AI
curricula have yet been implemented. However, increasing efforts are put into creat-
ing AI literacy frameworks and educational materials that can help teachers intro-
duce AI into their classrooms. For instance, in the United States, the AI4K12 project
identified  ‘Five  big  ideas  in  AI’  and  organized  K-12  frameworks  around  them,  to
serve as guidelines for AI curricula development (Touretzky et al., 2019). Research-
ers at the Massachusetts Institute of Technology (MIT) developed a collection of AI
lessons and tools to engage middle schoolers with the social and ethical implications
of AI (Williams et al., 2022). Similarly in Europe, the Erasmus + project GENERA-
TION AI developed an AI introductory course for primary school children available
in various European languages. Germany launched a national initiative that included
a 6-module AI course (Micheuz, 2020), and in the Netherlands, which defines the
context of this study, a series of ‘National courses are available for school teachers,
with different themes, such as AI in healthcare and AI in education.

As  these  projects  are  a  few  examples  of  pioneering  the  development  of  educa-
tional materials and implementing them is novel, consequently, there exists a gap in
the knowledge base regarding students’ understanding of AI, particularly as a socio-
cultural tool with ethical implications. This gap also exists in the context of primary
schools,  where  it  is  especially  relevant  because  of  the  wide  variety  of  young  chil-
dren’s  interests,  capabilities,  and  prior  experiences,  presenting  a  unique  challenge
to  curriculum  development.  To  design  effective  and  meaningful  AI  curricula  that
resonate with all learners, an in-depth understanding of children’s engagement and
sense-making of AI is needed to inform future curriculum design initiatives. This is
precisely what the purpose of the single case study reported in this paper is about; to
explore young children’s understanding of AI.

2   Theoretical and empirical underpinnings

2.1   AI literacy

AI literacy is part of the 21st-century skills that can enhance living standards, learn-
ing and working effectiveness, employability, and support citizens in becoming criti-
cal consumers of this technology (Ng et al., 2022a, b; OECD, 2021). The term “AI
literacy” was first coined by Burgsteiner et al. (2016) and Kandlhofer et al. (2016),

Education and Information Technologies

who defined it as the ability to understand the techniques and concepts behind AI-
driven applications and services, instead of just learning how to use them. A related
conceptual framework, inspired by drawing an analogy with classic literacy develop-
ment, contains four different stages of AI literacy development, where each stage is
assigned to an educational level starting with building awareness in primary school
to becoming fluent in AI at the university level. As the first conceptual framework
of AI literacy, the work started a conversation on what AI literacy is and what stu-
dents need to learn. Since then, other conceptual frameworks have been conceptual-
ized. What follows is a brief overview of three AI literacy frameworks, with each its
focus, advantages, and challenges.

A more holistic approach to AI is taken by Kong and Zhang (2021), by shedding
light  on  both  the  technical  and  socio-cultural  dimensions  of  AI.  The  authors  pro-
pose a conceptual framework for AI literacy to empower citizens by promoting AI
literacy and the ethical use of AI. The framework involves three dimensions. First,
the cognitive dimension, which focuses on skills and competencies needed to under-
stand  and  evaluate  AI.  Second,  the  affective  dimension  promotes  AI  self-efficacy
and AI perceptions to empower citizens by making them feel AI is personally mean-
ingful to them. The final dimension is the socio-cultural dimension, promoting the
ethical use of AI. Expanding the framework beyond the technical dimension of AI is
modern and AI attitudes and AI self-efficacy constructs to empower citizens receive
increasing attention in the literature (e.g. Schepman & Rodway, 2023; Wang et al.,
2023). However, Kong and Zhang do not offer a set of concrete competencies that
can be used to design AI literacy instructions or explore or assess their three dimen-
sions of AI literacy.

Focusing  more  on  cognitive  development,  Ng  et  al.  (2021)  conceptualized  AI
by  building  on  the  classic  educational  model  of  Bloom’s  taxonomy.  Through  an
explorative review, they reviewed how 30 studies define AI literacy and synthesized
these different AI literacy definitions into different levels of cognitive processes in
AI  learning,  each  associated  with  a  different  taxonomy  level.  These  levels  require
a  higher  level  of  complexity  and  ordered  thinking  from  students,  and  are  succes-
sive so that one level must be mastered before the next level can be reached. From
the  bottom  upwards,  Ng  et  al.  determined  the  levels  to  be  ‘Know  and  Understand
AI’, ‘Use and Apply AI’, and the highest level’Evaluate and Create AI’. Moreover,
they  conclude  emphasis  on  the  ethical  use  of  AI  is  needed.  With  this  framework,
Ng  et  al.  are  the  first  to  bring  classic  educational  models  to  AI  literacy.  Bloom’s
taxonomy  was  initially  created  in  the  1950s,  and  originally  it  endured  popularity
because it provides a structured framework for educators to design learning objec-
tives and assessments. However, some critics argue that the hierarchical structure of
the model is too rigid and that learning is more complex and dynamic, and individu-
als may engage in higher-order thinking without mastering lower-level skills first.

A more contemporary and widely used framework of AI literature has been devel-
oped by Long and Magerko (2020). Following a scoping study of existing research
to synthesize what AI professionals believe all citizens should know and common
perceptions  and  misconceptions  among  learners,  the  researchers  conceptualize  AI
literacy  through  five  overarching  competencies:  what  is  AI,  what  can  AI  do,  how
does AI work, how should it be used, and how is it perceived. Additionally, Long

Education and Information Technologies

and Magerko formulated 17 competencies that should be acquired to be AI literate
(Table 1). For educators, the conceptual framework comes with 16 design consid-
erations to support the design of AI education.

Although this conceptual framework offers concrete design guidelines and com-
petencies  for  educators,  it  is  predominantly  focused  on  the  technical  dimension  of
AI, as evidenced by the fact that 16 out of 17 competencies focus on technology. Lit-
tle attention is given to the socio-cultural dimension of AI.

In  the  pursuit  of  understanding  primary  school  students’  conceptualization  of
AI,  this  exploratory  study  adopts  a  comprehensive  approach  by  drawing  on  these
various conceptual frameworks and framing them within socio-cultural theories of
learning emphasizing social interaction. This approach is deemed appropriate for an
exploratory study, given that our purpose is not to test a predetermined hypothesis
but to generate insights that inform the design of personally relevant and meaningful
AI education materials for primary schools.

2.2   Children’s understanding of AI

An examination of the literature shows that before any AI education, young children
start conceptualizing AI through exposure to media representations of AI or experi-
ences  with  AI-enabled  devices  at  home  (Druga  et  al.,  2017;  Szczuka  et  al.,  2022;
Williams et al., 2019). Some children frequently engage with voice assistants such
as  Hey  Google  or  Siri.  Moreover,  their  interactions  extend  to  popular  digital  plat-
forms like TikTok and YouTube, where algorithms dynamically tailor online content
to align with their individual preferences. Additionally, children actively explore the
functionalities  of  generative  AI  models,  such  as  ChatGPT  or  DALL-E,  leveraging
these technologies to compose text and generate images. Collectively, these interac-
tions contribute to the development of their AI understanding and shape their atti-
tudes toward the technology (Ottenbreit-Leftwich et al., 2022) and hence AI concep-
tions, before starting AI education, can therefore vary as children can have different
experiences  (Druga  et  al.,  2017)  or  unequal  access  to  such  experiences  (UNICEF,
2021). To ensure effective teaching instructions for all children AI educational mate-
rials must tie into these varying prior experiences of children.

Empirical evidence shows that children’s existing understanding of AI is that it is
not a binary concept that can be determined to be right or wrong, as it encompasses
a range of perceptions. For example, in a study with 195 Finnish students aged 12 to
13, Mertala et al. (2022) showed that students’ initial conceptions of AI are varied
and often uninformed. Data were collected through an online qualitative survey, ask-
ing children to describe what AI is, what it is used for, how it works, and why they
think  AI  is  used.  The  students  described  AI  as  an  autonomous  system,  being  able
to  perform  tasks  on  its  own,  without  the  need  for  human  interference.  They  also
conceptualized  AI  as  programmed,  acknowledging  the  human  role  in  AI  develop-
ment.  The  same  students  also  showed  to  have  less  accurate  conceptualizations  of
AI;  assigning  AI  human-like  cognitive  processes,  such  as  knowing  and  thinking.
These conceptualizations imply an anthropomorphic conception of what AI is. This
is in agreement with the findings of a study conducted by Ottenbreit-Leftwich et al.

Education and Information Technologies

Table 1   AI literacy competencies adopted by Long and Magerko (2020)

Competency

Description

Theme: What is AI?

 1. Recognizing AI

Distinguish between technological artifacts that use and do not

use AI

 2. Understanding Intelligence

Critically analyze and discuss features that make an entity

 3. Interdisciplinarity

 4. General vs. Narrow

Theme: What can AI do?

 5. AI’s Strengths & Weaknesses

“intelligent”, including discussing differences between human,
animal, and machine intelligence

Recognize that there are many ways to think about and develop
“intelligent” machines. Identify a variety of technologies that
use AI, including technology spanning cognitive systems,
robotics, and ML

Distinguish between general and narrow AI

Identify problem types that AI excels at and problems that are
more challenging for AI. Use this information to determine
when it is appropriate to use AI and when to leverage human
skills

 6. Imagine the Future of AI

Imagine possible future applications of AI and consider the

effects of such applications on the world

Theme: How does AI work?

 7. Representations

Understand what a knowledge representation is and describe

some examples of knowledge representations

 8. Decision-Making

Recognize and describe examples of how computers reason and

make decisions

 9. Machine Learning Steps

Understand the steps involved in machine learning and the prac-

tices and challenges that each step entails

 10. Human role in AI

Recognize that humans play an important role in programming,

choosing models, and fine-tuning AI systems

 11. Data literacy

Understand basic data literacy concepts

 12. Learning from Data

Recognize that computers often learn from data (including one’s

data)

 13. Critically Interpreting Data

Understand that data cannot be taken at face value and requires

 14. Action & Reaction

 15. Sensors

Theme: How should AI be used?

 16. Ethics

interpretation. Describe how the training examples provided in
an initial dataset can affect the results of an algorithm

Understand that some AI systems have the ability to physically
act on the world. This action can be directed by higher-level
reasoning (e.g. walking along a planned path) or it can be reac-
tive (e.g. jumping backward to avoid a sensed obstacle)

Understand what sensors are, recognize that computers perceive
the world using sensors, and identify sensors on a variety of
devices. Recognize that different sensors support different
types of representation and reasoning about the world

Identify and describe different perspectives on the key ethical

issues surrounding AI (i.e. privacy, employment, misinforma-
tion, the singularity, ethical decision-making, diversity, bias,
transparency, accountability)

Theme: How do people perceive AI?

 17. Programmability

Understand that agents are programmable

Education and Information Technologies

(2022),  who  explored  the  existing  understanding  and  interests  of  young  learners
about AI in the context of the US. Through one-on-one semi-structured interviews.
Ten students, aged 9 to 11, were asked to elaborate on what AI means, and how it
works, and to share examples of AI. Nine of the ten described AI to involve coding
or programming. Moreover, the researchers noticed that at times, students described
AI to be preprogrammed, making it a human-made artifact, while other times, attrib-
uting anthropomorphic, life-like characteristics to these devices, using personal pro-
nouns to describe an AI, and thinking it could understand and see them. The idea
of AI being preprogrammed illustrates a lack of a deeper understanding of what AI
is and how it works. Ottenbreit-Leftwich et al. (2022) observed a few students who
understood AI involves training, however, none of the students were able to give a
more detailed explanation of how an AI model is trained.

Another set of studies provides evidence that students’ awareness of AI applica-
tions in their daily lives is nuanced and not easily characterized as entirely correct
or incorrect. While young children demonstrate proficiency in identifying numerous
AI applications (Mertala et al., 2022; Ottenbreit-Leftwich et al., 2022), such as voice
assistants, search engines, recommendation software, and autonomous devices, Kim
et al. (2023) observed students tend to over label automated appliances as instances
of  AI.  During  an  AI  summer  camp,  14  middle  schoolers  were  observed  playing  a
game  called  “AI  or  not  AI?”.  Students  were  shown  an  everyday  object  and  then
asked to position themselves from 0 (Not AI) to 100 (AI). Once situated, teachers
would  ask  students  to  share  their  reasoning  for  their  assessment.  Throughout  the
activity, several students argued that many automated instances were examples of AI
such  as  electronic  toll  collection  on  the  highway  using  RFID  technology,  washing
machines controlled by personal mobile phones using IoT, and industrial robots in
the factory.

Notably,  there  exist  conflicting  findings  concerning  students’  understanding  of
the  ethical  dimension  of  AI.  The  literature  reports  students  who  believe  AI  could
make lives easier, by helping with certain chores. Especially the lives of sick or dis-
abled  people,  who  could  benefit  from  such  AI  applications  (Mertala  et  al.,  2022;
Ottenbreit-Leftwich et al., 2022). Regarding ethical risks, few students in the Otten-
breit-Leftwich et al. (2022) study, shared concerns regarding cybersecurity, privacy,
and misuse of AI applications. On the contrary, Kim et al. (2023) found students not
to worry about AI, believing AI is emotionless and therefore impartial and fair, not
recognizing any ethical considerations of AI.

Collectively, the findings of existing studies shed light on the complexity of stu-
dents’  existing  understanding  of  AI  that  they  bring  into  the  classroom.  AI  under-
standing is not a binary concept that can be determined to be correct or incorrect.
Instead, it encompasses a range of perceptions, as illustrated by the above-described
empirical findings. Research also shows cultural upbringing can influence children’s
perceptions and attitudes toward AI (Long & Magerko, 2020), adding to the com-
plexity of AI literacy development. In recognition of this complexity, in this study,
we  use  a  qualitative  research  approach  to  explore  primary  school  students’  AI  lit-
eracy in depth.

Education and Information Technologies

2.3   AI‑based educational practices

Education,  governments,  and  industry  leaders  have  acknowledged  the  value  of
promoting AI literacy among young learners in primary and secondary education
(Pedro et al., 2019; Touretzky et al., 2019; UNESCO, 2019). There are currently
14  AI  curricula  endorsed  and  implemented  at  primary  and  secondary  school
levels  by  governments  in  11  countries  (UNESCO,  2022).  Additionally,  scholars
started  to  design  and  implement  AI  learning  activities  in  the  form  of  interven-
tions,  workshops,  or  courses,  and  empirical  studies  on  how  such  teaching  and
learning efforts can support students in fostering AI literacy. For example, Zhang
et  al.  (2022)  designed  and  implemented  the  “Developing  AI  Literacy”  (DAILy)
course  that  aimed  to  develop  AI  literacy  among  middle  school  students  by  cov-
ering  age-appropriate  technical  knowledge  and  skills  in  AI,  creating  an  under-
standing  of  the  ethical  and  societal  implications  of  AI,  and  finally,  by  increas-
ing knowledge of AI’s impact on jobs. A group of MIT researchers designed and
pilot-tested various AI Ethics activities for middle school students (e.g. Ali et al.,
2021; DiPaola et al., 2020; Lee et al., 2021; Payne, 2020; Williams et al., 2022).
Van Brummelen et al. (2021) used Long and Magerko’s (2020) conceptual frame-
work to design an online workshop of five days. Williams et al. (2022) pilot-tested
three  activities  on  generative  adversarial  networks  (GANs),  deep  fakes,  coding,
and supervised machine learning. The activities were meant to increase students’
technical  AI  knowledge,  ability  to  think  critically  about  the  implications  of  AI,
and  ability  to  apply  AI  knowledge  to  topics  they  personally  care  about.  Eguchi
et  al.  (2021)  used  related  MIT-designed  AI + ethics  curricula  and  used  a  cultur-
ally  responsive  approach  to  teach  it  by  adapting  the  curricula  to  the  Japanese
context. To do this, they changed or added certain AI application examples. For
example,  YouTube  was  replaced  with  the,  in  Japan  wider  known,  Yahoo.  They
also added various AI-enhanced devices that are well-known and used in Japan,
such as AI-enhanced microwaves, washing machines, and personal service robots.
Moreover,  various  pedagogical  approaches  such  as  problem-based  learning  (Su
& Zhong, 2022), role-playing (Henry et al., 2021), and digital story writing (Ng
et al., 2022a, b) were introduced to develop young children’s AI literacy.

This body of research provides insights into the design of AI literacy education
materials,  and  the  pedagogical  strategies  tools,  and  resources  used  to  implement
these  curricula.  A  critical  examination  of  the  knowledge  base  showcases  specific
gaps:  a)  lack  of  qualitative  studies,  b)  lack  of  studies  in  the  context  of  Europe,  c)
few studies explore learning experiences and processes, and d) few studies approach
AI as a socio-cultural tool. Overall, the existing knowledge base offers merely pre-
liminary insights into the impact of specific curricula on children’s AI literacy. The
effects are often examined by focusing on learning outcomes, using mixed or quanti-
tative methods. Qualitative research methods can mostly be found in conference pro-
ceedings,  but  offer  only  superficial  preliminary  findings  of  pilot  tests  of  curricula.
We argue that since only recently AI curricula have been implemented, more atten-
tion  should  be  paid  to  qualitatively  understanding  children’s  understanding  of  AI
and learning experiences rather than solely focusing on learning outcomes. This is
precisely the purpose of this manuscript.

Education and Information Technologies

2.4   Research questions

As stated earlier, there exists a lack of qualitative studies exploring children’s under-
standing of AI that go beyond the technical dimension and address the socio-cultural
nature of AI. In attempting to address this gap in the knowledge base, in the study
reported  in  this  manuscript  we  explore  young  children’s  understanding  of  AI  as  a
technology, AI as a tool, and AI as an ethical phenomenon. In doing so, we aim to
respond to the following research questions:

RQ1: How do young children understand AI?
RQ2: How do young children understand AI applications in their daily lives?
RQ3: What (if any) ethical risks do young children identify in relation to AI?

In responding to these questions, through this study, we aim to contribute to the gap
in existing literature on an in-depth understanding of students’ AI conceptualizations.
We provide detailed insights into students’ engagement with AI that can be used to sup-
port the design of effective and engaging AI curricula for primary school children.

3   Methods

3.1   Research design

To  explore  what  AI  themes  and  competencies  are  appropriate  for  primary  school
children  we  follow  a  qualitative  case  study  paradigm  with  the  case  being  defined
by  a  group  of  young  children  in  a  school  classroom  (Merriam  &  Tisdell,  2015).
Adopting this paradigm allows us to employ a detailed and in-depth exploration of
students’  understanding  of  AI.  Even  though  the  students  in  the  classroom  will  be
treated as one case, in analyzing the data we apply a constant comparative approach
across the students to also gain an understanding of their personal understanding.

3.2   Context and participants

The  context  of  the  study  was  defined  by  a  primary  school  classroom  of  18  chil-
dren (11–12 years old). The classroom was selected through a purposeful sampling
method to ensure: a) the classroom population is representative of the country’s stu-
dent  population,  and  b)  the  teacher  has  background  knowledge  and  interest  in  the
use of AI tools in teaching and learning. In terms of demographics, typical of the
country’s  population,  the  classroom  included  Caucasian  (national)  children  and  a
few of those with a migration background, mostly Arabic. There were nine boys and
nine girls, all with average to high socio-economic status. We purposefully selected
a classroom with a teacher who was knowledgeable enough about AI and was com-
fortable  in  implementing  an  introductory  course  specially  designed  for  this  age
group.  The  study  meets  the  ethical  and  legal  requirements  of  the  university  in  the

Education and Information Technologies

country context where this study is carried out. Approval for the study was received
following an application process to the Ethics committee, which included informa-
tion about data privacy, consent, confidentiality, and data protection.

The teacher has more than five years of experience, he has a strong interest in digital
literacy including AI and he has been involved in curriculum design for digital literacy.
The teacher implemented a five-week-long AI introductory course, based on an AI cur-
riculum developed and pilot-tested as part of an EU-funded project developed by the
researchers. The course includes 5 lessons on AI definition, voice assistants, machine
learning, and AI ethics and is centered around the digital learners’ competencies formu-
lated by the Digital Competence Framework for Educators (Redecker, 2017).

It is important to note that the course was not used as an intervention but instead as
a context for initiating conversations with the students about AI that would help shed
light on their existing understanding of AI. This is precisely why the course was not
intended to support the construction of new knowledge but to engage students in activi-
ties that would support them in making their prior knowledge and thinking visible.

3.3   Data collection and analysis

The main research data for this study were collected through five online group inter-
views  with  the  children.  Conducting  online  interviews  with  children,  as  opposed
to in-person interviews, not only fosters a sense of comfort in their familiar digital
environment but also serves as a strategic method to mitigate traditional adult–child
power dynamics. In the virtual setting, children can feel safe to express themselves.
An illustrative example of this dynamic emerged during interviews where children
desiring  a  private  exchange,  muted  themselves  for  a  few  seconds  to  communicate
with each other without the direct presence of the interviewer. This instance show-
cased a self-directed control over the interview space, reflecting a nuanced shift in
power  dynamics  that  supports  children  to  communicate  freely  within  the  digital
context. To address limitations of the online interaction in forming bonds with the
children, we started each interview with informal conversations that included pres-
entations of ourselves in regards to personal interests and hobbies. To increase our
efforts to make children, especially shy ones, feel at ease during the interviews, we
chose group interviews. Each interview lasted between twenty and thirty minutes.

The first author conducted the interviews and acted as a facilitator to create a comfort-
able atmosphere in which all participants felt free to share their voices. The second author
has more than 15 years of experience with qualitative research, both as a researcher as
well as an instructor of advanced qualitative methods courses at the doctoral level. The
second author provided training to the first author in interviewing techniques to overcome
common limitations of group interviews, such as dominant personalities, silent partici-
pants, or students getting off-topic. Techniques such as (a) directing questions at less par-
ticipating students, (b) follow-up questions on non-verbal communication from students
(e.g. nodding, facial expressions), and (c) naturally redirecting the conversation.

The  interviews  were  semi-structured,  allowing  a  more  natural  conversation  to  take
place  during  which  the  students  played  a  role  in  navigating  the  direction  of  the  inter-
view. The interview protocol that was used, has been used in similar research with young

Education and Information Technologies

children in a different context (Avraamidou, 2013; Heeg et al., 2022). It included a com-
bination of closed and open-ended questions such as the following: Can you explain what
AI is? What are examples of AI? Is AI always good for us? A complete list of questions
can  be  found  in  Appendix.  The  interview  questions  were  developed  through  using  the
research question as a basis as well as input from the literature. They were discussed with
a research group and pilot tested with a small group of students for validity purposes.

During  the  interview,  the  children  shared  one  laptop  with  a  built-in  camera  to
communicate with the interviewer. All students’ upper bodies and heads remained
visible during the interview, to ensure non-verbal communication could be seen and
acted on by the interviewer. All interviews were audio tape-recorded and fully tran-
scribed for the purposes of the analysis.

For  the  data  analysis,  we  adopted  a  thematic  approach  to  identify  the  AI  literacy
constructs that were more salient. The Atlas software was used to analyze the data. To
carry out the analysis, we combined deductive and inductive analytic practices to pro-
vide a more comprehensive view (Vanover et al., 2022). For the first round of coding,
we applied a deductive approach to organize our data into deductive categories aligned
with the conceptual framework of Long and Magerko (2020), in this case, the AI lit-
eracy themes (see Table 1). During the second round of coding, we applied an induc-
tive approach, using in vivo, line-by-line coding techniques, to allow codes and patterns
to emerge from the data. Based on the patterns, the inductive codes were clustered in
categories, for example, ‘imagining the future with AI’, ‘opinions on AI’, and ‘privacy’,
which were then grouped under three themes aligned with the three research questions.
For an overview of the coding process, including examples of codes, please see Table 2.

3.4   Trustworthiness

In  pursuit  of  establishing  trustworthiness,  we  increased  the  internal  validity  of  the
interviews by incorporating different questions addressing the same issue in the inter-
view protocol. To establish external validity, both authors independently engaged with
the  data,  using  the  established  coding  scheme.  In  comparing  the  two  codings  there
was  an  80%  agreement.  The  authors  met  multiple  times  to  discuss  the  coding  and
interpretations.  The  authors  discussed  disagreements  until  a  consensus  was  reached
as a way to enhance the trustworthiness of the findings. However, typical with qualita-
tive research, confirmability and generalizability are not possible. We, however, pro-
vide detailed information about both the context and processes of data collection to
advance the possibility of transferability of the findings to similar contexts.

4   Findings

The outcomes of the analysis are presented around the final three themes that align
with the research questions of this paper: children’s understanding of AI, AI in daily
life, and ethical risks of AI. All children’s quotes were translated into English from
their original language.

Education and Information Technologies

s
e
m
e
h
t

e
e
r
h
t

r
e
d
n
u

s
e
i
r
o
g
e
t
a
c

g
n
i
p
u
o
r
G

g
n
i
s
u

s
e
i
r
o
g
e
t
a
c

o
t
n
i

s
e
d
o
c

g
n
i
r
e
t
s
u
l
C

-
y
b
-
e
n
i
l

,
o
v
i
v
-
n
i
g
n
i
s
u
g
n
i
d
o
c
f
o
2
d
n
u
o
R

5

e
h
t

g
n
i
s
u

g
n
i
d
o
c

e
v
i
t
c
u
d
e
d
f
o

1

d
n
u
o
R

s
n
o
i
t
s
e
u
q

h
c
r
a
e
s
e
r

e
e
r
h
t

e
h
t

h
t
i

w
d
e
n
g
i
l
a

f
o

s
e
i
c
n
e
t
e
p
m
o
c

7
1

e
h
t

o
t

g
n
i
d
d
a

d
n
a

:
s
e
d
o
c
e
l
p
m
a
x
E

.
s
e
u
q
i
n
h
c
e
t
g
n
i
d
o
c
e
n
i
l

o
k
r
e
g
a
M
d
n
a

g
n
o
L
y
b

s
e
m
e
h
t

y
c
a
r
e
t
i
l

I

A

e
l
p
m
a
x
E

.
)
0
2
0
2
(

o
k
r
e
g
a
M
d
n
a

g
n
o
L

:
s
e
i
r
o
g
e
t
a
c

)
0
2
0
2
(

s
s
e
c
o
r
p

g
n
i
d
o
c

f
o
w
e
i
v
r
e
v
O

2
e
l
b
a
T

s
e
v
i
l

y
l
i
a
d

’
s
t
n
e
d
u
t
s

n
i

I

A

I

A
g
n
i
z
i
n
g
o
c
e
R

-
a
d
n
e
m
m
o
c
e
r

I

A

,
e
b
u
T
u
o
Y

,

m
h
t
i
r
o
g
l
A

?
I
A
s
i

t
a
h
W

s
n
o
i
t

s
e
v
i
l

y
l
i
a
d

’
s
t
n
e
d
u
t
s

n
i

I

A

I

A

f
o

g
n
i
d
n
a
t
s
r
e
d
n
U

I

A

f
o

s
k
s
i
r

l
a
c
i
h
t
E

s
e
v
i
l

y
l
i
a
d

’
s
t
n
e
d
u
t
s

n
i

I

A

I

A
n
i

e
l
o
r

n
a
m
u
H

I

A
g
n
i
k
a
m

,

d
e
m
m
a
r
g
o
r
p

s
i

I

A

I

A

f
o

e
s
U

p
l
e
h

n
a
c

I

A

,
k
l
a
t

n
a
c

I

A

,
n
a
e
l
c

n
a
c

I

A

?
k
r
o
w

I

A
s
e
o
d
w
o
H

?
o
d

I

A
n
a
c

t
a
h
W

n
o
i
n
i
p
O

s
c
i
h
t
E

y
c
a
v
i
r
p

,
t

fi
o
r
p

,

m
s
i
x
e
s

,

m
s
i
c
a
r

,
s
a
i
B

?
d
e
s
u

e
b

I

A
d
l
u
o
h
s
w
o
H

n
o
i
n
i
p
O

?
I
A
e
v
i
e
c
r
e
p

e
l
p
o
e
p

o
d
w
o
H

Education and Information Technologies

4.1   Children’s Understanding of AI

After  a  brief  conversation  on  whether  or  not  the  students  enjoyed  the  AI  lessons,
the  researcher  opened  the  interviews  by  asking  if  students  could  explain  what  AI
is. This question led to some initial discomfort in most groups, manifesting in the
form of silences, exchanged glances, or laughter. In some cases, students would even
explicitly  share  that  this  was  a  difficult  question  to  answer.  However,  in  the  end,
most groups provided a collective answer, that all group members contributed to and
agreed with. For example, Sascha, Julia, Feline, and Rachel shared:

Researcher: Could you try to explain to me what AI is?
Sascha: Julia can start.
Researcher:  There are no right or wrong answers, everything is ok.
Julia: Well, ehm…
[…]
[all group members laugh]
Julia: It is hard to explain
Sascha: Shall I start?
Julia: Yes please. Sascha will start.
Sascha: Well, I will try. AI is actually, well, a device or something, which has been developed by humans but which can actually
make its own decisions.
Researcher: okay
Sascha: Something like that…
Researcher: Does anybody want to add something to that? Or, has other ideas about what AI is?
Feline: Maybe that they can decide that, for example, a self-driving car, they can decide where they want to go. Well, not exactly
decide, but they can describe the route and then decide which one to take.
Rachel: They choose themselves
Julia: They can think on their own, but they are programmed, so it is not actually by themselves.
Researcher: okay,, so they can think on their own, and decide on their own.
Julia: Yes

As apparent in the extract, after Sascha started answering the question, all other
group  members  felt  comfortable  adding  to  the  explanation.  As  can  be  seen  in  the
extract,  the  group  described  AI  as  a  device  that  is  programmed  or  developed  by
humans and that can think and decide on its own. This is a typical description that
occurred more or less in all group interviews. Some other typical descriptions from
the  interviews  were  AI  as  an  ‘algorithm’,  or  ‘computer  system’,  that  is  ‘smart’  or
‘intelligent’ because it can ‘learn’, ‘think’, or ‘do things’ on its own. For example,
when Bobby was asked why he called AI ‘smart’, he answered:

Bobby: Well, it can do a lot of things. AI can do things by itself. I think that is smart of AI.

In one group, all group members reacted to the researcher’s question by turn-
ing their heads to Fred who interpreted this non-verbal behavior as inviting him
to  answer  this  question.  At  the  beginning  of  the  interview,  Fred  already  posi-
tioned himself as an expert on AI and felt comfortable answering this question
on his own. His group members agreed with his explanation, but didn’t add any-
thing to it:

Education and Information Technologies

Researcher: Can you tell me what AI is?
[all group members looked at Fred]
Fred: Well, I think I will take this one if everybody is looking at me. But I don’t mind. Artificial Intelligence,  also known
as AI, is a computer or a computer program that can independently perform certain tasks, independently decide things,
or decide how to do them. For example, when you say something to a Google Assistant, like ‘Hey Google, turn on the
lights’, it will turn on the lights.
Researcher: Okay. Lars, do you want to add something to that?
Lars: No, I think he said everything.

Although  all  children  seemed  to  understand  that  AI  can  ‘think’  or  ‘learn’
by  itself,  nobody  was  able  to  answer  follow-up  questions  such  as  ‘how  does  it
think?’ or ‘how does it learn?’. Nevertheless, students had some misconceptions
about recognizing Artificial Intelligence, as can be seen in the following conver-
sation prompted by the researcher asking if the students have AI at home:

Erica: I have a phone and my sister has a tablet. My father has two phones. One for work and one for at home. And my mum has a
phone. I have another one.
Researcher:  And, do all mobile phones use AI?
Stella: Yes, I think so.
Erica: yes
Stella: well actually, when you ask it like that, I don’t think it has.
[Stella and Erica laugh]
Stella: oh yes, a phone is electric

The extract shows how both Erica and Stella think that all phones use AI, later
doubting that. Similarly, in a different group, Fred and Lars suggested automated
calendar notifications might also be instances of AI, but weren’t sure about this:

Researcher: How do you use AI at home?
Fred: Well for example, ehm…, for example when you cannot remember things very well, then you can look things up. For example,
when my birthday is, or a calendar, an online calendar. My grandma does that sometimes.
Lars: Yes, very helpful as a calendar
Researcher: And what do you mean by calendar? What is AI about the calendar?
Lars: For example, a calendar. Well, I am not sure if that is AI.
Lars: When you put something on your calendar, then it will send you a reminder one day in advance of when that will happen. I
am not sure if that is AI

As apparent in this dialogue, students tend to label electronic devices or auto-
mated functions, such as online calendars and reminders, as instances of artifi-
cial intelligence, reflecting a limited understanding of AI.

4.2   AI in daily life

When  prompted  to  share  examples  of  AI,  all  groups  stated  the  most  common  AI
applications that children in the context of the study can come across in daily life.
Examples included recommendation algorithms on social media and Netflix, voice
assistants  like  Alexa  and  Hey  Google,  robotic  vacuum  cleaners  and  lawnmowers,
and self-driving cars. For example, Rachel, Feline, Sascha, and Julia shared:

Education and Information Technologies

Researcher:  And do you know examples of AI?
Rachel: self-driving cars, Siri, Alexa, Google Assistant, Corta,
Feline: Corta?
Rachel: that’s from Windows
Sascha: Netflix
Researcher:  And what exactly is AI on Netflix?
Sascha: Well, for example
Julia: Cookies
Sascha: No,  but if you have watched a certain series, then it tells you, you may also like this’
Feline: and it says you can watch this, examples of what you can watch, and usually there is something fun in there that you want
to watch
Sascha: because you,
Feline: because you already watched certain things, and because you watch things, or you searched for specific things, it suggests
things that are kind of similar. Yes.
Julia: Also on TikTok
Feline: TikTok and Snapchat

In all groups, children named Netflix, TikTok, and YouTube as AI. Through fol-
low-up  questions,  they  would  specify  how  such  platforms  use  AI  to  identify  what
videos they like to suggest content of a similar nature. For example:

Betty: On TikTok for example, when you watch a certain video, and you like it, that the next few videos will be the exact same videos.
Which can be strange as well…
Researcher: Could it also be boring?
Betty, Sarah, Rose: Yes!
Rose: You never get something different
I: And do you have solutions for this? Do you ever use each other’s phones to see something else?
Rose: when you press ‘not interested’ you will see those videos less
Sarah: yes
Researcher: Oh, I didn’t know that was possible
Rose: Yes, and you can also, I do this sometimes, when I don’t like a video, I would like it, so I would get other types of videos.
Rose: Or, if you scroll over a nice video, or don’t watch it until the end
Betty: yes, that’s right

In this dialogue, Sarah, Betty, and Rose describe how they interact with AI sys-
tems when they watch videos on TikTok. All three shared their scrolling and liking
behavior to manipulate the AI in providing other types of TikTok content. In another
group, Tom and Erica, shared similar strategies for manipulating the algorithm:

Tom: What I have done is like a video [on TikTok] that I didn’t like, because that would give me other types of videos.
Researcher:  you mean you are tricking the AI system in giving you other content?
Tom: yes, and for example, immediately scrolling through videos that you like.
Erica: Yes, that is right.
Tom: Or when you watch a video, but don’t finish it, or don’t watch it for a long time.

This  particular  use  of  the  AI  recommendation  software  shows  a  good  under-
standing of how such platforms track their behavior (e.g. likes, scrolling, watch-
ing)  and  use  it  to  provide  tailored  content.  However,  children  did  not  explicitly
recognize  AI  recommendation  algorithms  responsible  for  this  function;  rather,
they  generally  attributed  this  capability  to  the  platforms,  such  as  Netflix  and

Education and Information Technologies

TikTok. When asked to share what they thought about AI recommendations, chil-
dren reported mixed feelings: both boring and convenient.

Besides  experiences  with  recommendation  software,  experiences  with  voice
assistants were shared in all groups, showing examples of how children use voice
assistants. For example:

Aron: We had a Hey Google at home
Researcher: And what would you use it for?
Aron: Ehm,… Usually, for example, to change the volume of music. Asking it for jokes is also fun.
Bobby: ‘Hey Google, a joke.’ And then it will tell you a joke.

As the extract shows, Aron used the voice assistant at his home for both prac-
tical  tasks  and  entertainment.  In  three  groups,  students  also  shared  how  they
believe  voice  assistants  can  be  helpful  for  sick,  disabled,  or  lazy  people.  For
example, Ruben, Bobby, and Julia all said similar things in different groups:

Ruben: I think AI is convenient for lazy people, or people who are sick, because they can close the curtains with their voice.

Bobby: It can help you with things. For example, with things that some people cannot do on their own. For example, people who
have two broken legs, or no legs. Ehm… Yes, it can help them by automatically closing the curtains, or turning on lights.

The  most  common  tasks  that  the  students  reported  using  voice  assistants  for
were related to listening to music, turning on and off the lights, opening and clos-
ing windows, or telling jokes.

All groups mentioned robot vacuum cleaners and lawnmowers as an AI exam-
ple. A few students shared that they had these applications at home, while others
mentioned they had seen them at friends’ houses. Similar to the voice assistants,
students thought such robots could be helpful:

Stan: A robot lawn mower is pretty convenient too. It can save you time.

Julia: I think the same as Sascha. It can be convenient, for example, if you are sick and you cannot vacuum every day, a robot  can
do it for you. Or a robot lawn mower. But it can also be used in a bad way.

4.3   Ethical risks of AI

During  the  interviews,  students  extensively  talked  about  various  ethical  issues.
In the introductory course, one lesson specifically discussed the topic of AI and
Ethics. In this lesson, the teacher discussed with the students why platforms such

Education and Information Technologies

as YouTube and TikTok use AI recommendation software. During the interviews,
it became evident students have a clear understanding and strong opinions on this
matter.

Stella: you know that YouTube gets money when we are watching videos. So they make money off my data.
Researcher: You mean they make money when you use YouTube?
Stella: Yes, I think they should share their profits with me. Because it is my data. They have things that are mine, it is about me.
Researcher: so how does it work?
Stella: Well, if you are watching videos that you don’t like, you will probably stop watching videos on YouTube. So you will have
less screen time, and then YouTube will get less money because of that.
Researcher: and you think it is not fair that they make money through you?
Stella, Erica, and Tom [simultaneously]: Yes!
Stella: The money should come to us, they already have enough money, why don’t they share it? It is our money. No, it is our data,
and they get the money.
Researcher: ok
Stella: Do you get it? I mean, I don’t necessarily have to make money, but that they make money with my data, that’s not ok

Stella  understands  that  YouTube  used  the  algorithm  to  make  sure  she  spends
more  time  on  their  platform  which  will  eventually  make  her  behavior  profitable
for YouTube. Stella and her group members believe it is unfair that YouTube uses
her data to make a profit, touching upon the ethical issue of ownership. Stella felt
very strongly about this, even asking the researcher in the end ‘Do you get it?’ to
make sure her point had come across. Another group had fewer problems with the
idea that YouTube makes money by using this algorithm:

 Researcher: And why do you think YouTube or TikTok use AI?
Rachel: so you will keep watching videos
Feline: Yes, they make money, so when you watch more videos, they make more money
Sascha: commercials, because of the commercials. You watch more videos and also more commercials, and they earn money with
that
Researchers: And what do you think about that? That they earn money.
Sascha: it’s a little annoying, but in some way it is convenient
Feline: Yes
Julia: I mean, they have to eat too. I think it is smart. They have to earn money too.
Rachel: I don’t mind it. I mean, I am not complaining when my mum is going to work to make money, right?
Feline: I think it is pretty smart. Smart that they came up with this idea. Because we watch more videos so that they can make more
money. Very smart of those people.

In this group, all children believed it was YouTube’s right to earn money and they

didn’t feel they needed to be compensated for that.

A second ethical issue they were concerned with is privacy issues, being afraid
that personal data might be sold to other parties or used for other purposes than for
recommending new videos. For example, Rogier shared:

Rogier: Well, AI is governed by very big companies and you cannot always trust those.
Researcher: And why, what do you mean by ‘we cannot trust those’?
Rogier: Well, Facebook, for example, they have a lot of private data, and they use it for commercials, but that data can be easily
used for bad things.

Rogier shows a certain distrust of big tech companies, stating that such compa-

nies might misuse their private data. Sarah had similar worries:

Education and Information Technologies

Sarah: I think it is important that our personal information is not used to make money [by selling it to other companies], but only
to recommend new videos, but they [YouTube] cannot misuse our data.
Researcher: And what do you mean with personal data?
Sarah: Ehm.. Like your address and phone number.

Another  ethical  issue,  extensively  present  in  the  data  is  that  of  AI  bias.  All  the
children  talked  a  lot  about  the  issue  of  AI  bias.  During  the  AI  and  Ethics  lesson,
the students took part in an exercise where they would search for biases in Google
Images.  As  an  example,  the  teacher  googled  ‘baby’,  and  the  students  noticed  that
Google  would  mostly  show  Caucasian  babies.  After  this  example,  children  had
to  search  for  such  biases  in  Google  Images  themselves.  During  the  interviews,  it
became clear that children were unfamiliar with the term AI bias but did remember
the concept vividly.

Researcher: I believe you did an exercise where you had to look for images on Google. Your teacher used the example of googling
‘baby’ and you would only see white babies.
Rose: Ah, yes, we talked about this.
Betty: yes!
Sarah: we did this!
Researcher: Do you remember the lesson? What happened?
Rose: I remember this because I thought this lesson was super interesting. We talked about, for example, if you have face id, it
works better on white people than people of color. And for example, if you look for pictures of ‘fights’, you usually get pictures of
people of color instead of white people.
Sarah: And also with ‘kidnapping’ you …
Rose: yes
Sarah: you will see mostly men, but they could also be women.
Researcher: And do you know why this happens?
Betty: ehm…
Sarah: Because maybe this sounds more logical to them. It is the same in cartoons.

Other groups shared other examples of biased results, for example that search-
ing for ‘married’ mostly showed heterosexual couples and not ‘boy with boy or
girl with girl’. Some children explicitly called such biases ‘racist’ and ‘sexist’. For
example, Aron shared:

Aron: I think AI is racist, because when you search for ‘monkey holding a box’ [in Google Images], you will see a baby of color
with a box.

All  groups  disapproved  of  such  biases  and  believed  they  needed  to  be  fixed.
and  that  they  should  be  fixed.  However,  when  asked  about  the  origins  of  such
biases  or  potential  solutions,  a  large  majority  of  students  struggled  to  respond.
One exception was Stella, who shared an idea:

Education and Information Technologies

Researcher:  So how do you think we can change this [biases in Google Images]
Stella: well, that more people from, more foreigners, and people who are attracted to other genders, also just start working on  AI.
I don’t think it’s meant to be that way, because there are those white people who work on it and they kind of forget about the other
people. So I think that also just a little more foreigners can contribute to the research or whatever happens. I think maybe more
foreigners can join. Because look, for example, you don’t see Moroccans there on Google when you search for people. You don’t
see it.

Stella’s words ‘or whatever happens’ indicate that she does not understand how AI
algorithms are exactly made, but she realizes that there are people who develop such AI
algorithms. More specifically, Stella believes that there are mostly ‘white people’ who
work on such algorithms who unconsciously forget that there are other-looking people,
or in Stella her words ‘foreigners’. As a solution to AI bias, Stella believes development
teams should be more diverse to make sure nobody is overlooked or not represented. In
saying this, Stella places the responsibility for such biases on the developers and holds
them  accountable  for  this  issue.  Other  ethical  topics  that  were  briefly  mentioned  by
children were: fake news, misinformation, AI taking over the world, and people becom-
ing too dependent, and therefore lazy because AI takes over too many tasks.

5   Discussion

This study aimed to explore young children’s understanding of AI as both a tech-
nology and a socio-cultural tool. Through semi-structured group interviews, chil-
dren  were  inquired  about  what  AI  is,  how  they  use  it,  and  the  risks  they  see  in
using it. The findings are presented with accompanying data extracts to provide
insights  into  students’  views  and  are  structured  around  three  main  themes:  AI
understanding, AI awareness, and AI ethics.

The findings carry various similarities with recent studies on students’ prior AI
knowledge  and  conceptions.  The  first  similarity  is  that  on  an  abstract  level,  stu-
dents conceptualize AI as a computer system that is programmed by humans and
is intelligent because it can carry out tasks independently or autonomously engage
in  decision-making  (Henry  et  al.,  2021;  Mertala  et  al.,  2022;  Ottenbreit-Leftwich
et al., 2022; Williams et al., 2022). This abstract conceptualization lacked depth, as
evidenced  by  students’  misconception  of  labeling  electrical  devices  or  automated
processes  as  AI  (see  also  Kim  et  al.,  2023;  Williams  et  al.,  2022;  Zhang  et  al.,
2022) and a lack of understanding of how AI works (see also Mertala et al., 2022;
Ottenbreit-Leftwich et al., 2022).

In terms of AI use, the findings reveal a collective awareness among all students
regarding common AI applications in their daily lives, such as voice assistants, rec-
ommendation  algorithms,  and  robot  vacuum  cleaners.  Students  recognized  AI’s
potential in assisting with household chores and physical tasks, while also acknowl-
edging  its  capacity  to  help  individuals  with  disabilities  or  illnesses.  These  results
contribute to existing research, indicating that children perceive AI as a facilitator,
streamlining  tasks,  saving  time,  and  offering  support  to  those  less  abled  (Mertala
et al., 2022; Ottenbreit-Leftwich et al., 2022; Zhang et al., 2022).

Education and Information Technologies

The findings show high engagement with and nuanced awareness of the ethics of
AI.  Students  expressed  concerns  about  AI  bias  such  as  discrimination  against  peo-
ple  of  color.  Moreover,  students  actively  shared  opinions  about  issues  of  privacy
and data ownership, drawing on personal experiences with platforms like YouTube
and TikTok. Zhang et al., (2022) found similar results among slightly older students
(11–14 years). Similarly, Dipaola et al. (2022) found nuanced views on the ethics of
AI when it concerns AI technologies that students aged 11 to 14 are familiar with.
Another  general  concern  of  students  regards  the  increasing  reliance  on  AI,  conse-
quently making people lazy (Mertala et al., 2022; Zhang et al., 2022).

A  notable  difference  between  our  findings  from  the  existing  literature  is  the
absence of AI understanding as robots (e.g., Kim et al., 2023; Mertala et al., 2022;
Ottenbreit-Leftwich  et  al.,  2022;  Williams  et  al.,  2022;  Zhang  et  al.,  2022).  As
a  matter  of  fact,  no  one  used  the  term  "robot"  in  abstract  AI  conceptualizations.
References to robots were only limited to specific AI examples like robot vacuum
cleaners, lawnmowers, or two distinct provided by two individual students: educa-
tional toys or companionship robots in the home of a grandmother with dementia.
In  literature,  conceptualizations  of  AI  as  robots  are  often  present  and  assigned  to
unrealistic AI media portrayals (DiPaola et al., 2022; Kim et al., 2023). This might
suggest a more nuanced understanding of Artificial Intelligence.

A  possible  explanation  for  the  absence  of  robot-related  references  may  stem
from  the  fact  that  in  socio-cultural  context  of  the  study  it  is  uncommon  to  have
interactions with robots, whereas for example in Japan, personal and service robots
co-exist with people in Japanese society, where robots work together with people
in restaurants and shops (Eguchi et al., 2021). Another possible explanation of this
discrepancy might be that children’s understanding was dominantly framed within
the context of their own experiences and use of AI within their homes, whereas for
example in Zhang et al’s. (2022) study, students engaged in ways in which careers
will be impacted by AI, resulting in kids imagining AI being able to take over dan-
gerous jobs or perform tasks with more efficiency.

Our findings suggest that children develop an understanding of AI through the use
of AI technologies in their everyday lives. This is demonstrated in the findings through
children’s  various  strategies  to  manipulate  TikTok’s  recommendation  algorithms  to
curate diverse content, such as intentionally liking videos that they don’t enjoy. Although
children do not explicitly link these actions to the workings of AI, students’ everyday
experiences and observations lay the groundwork for a preliminary understanding. This
underscores the influential role of personal encounters in shaping students’ early percep-
tions of AI, aligning with the socio-cultural framework proposed by Vygotsky.

Furthermore,  our  findings  indicate  that  children  perceive  AI  as  first  and  foremost
as a tool rather than a technology. Unlike framing AI abilities in terms of its technical
capabilities, such as sensing or hearing through sensors, students focused on how AI
functions in the context of their homes. Examples included AI abilities to manage lights
and curtains, perform cleaning activities, or aid less abled in the context of their homes.
This perspective highlights the practical, hands-on role students attribute to AI, empha-
sizing its utility as a tool within their immediate living environments or those of others.
Finally,  the  findings  indicate  that  children  exhibit  a  high  level  of  engagement
with  the  ethics  of  AI,  showing  a  keen  interest  in  the  socio-cultural  implications,

Education and Information Technologies

particularly about AI applications with which they are familiar. Platforms like You-
Tube and TikTok provide appropriate and relatable contexts to start discussions on
crucial topics such as privacy, data ownership, and how AI influences behavior.

6   Conclusions and recommendations

The rapid integration of AI in our daily lives has turned AI literacy into a necessary
skill set and kicked off the development of AI education for schools. To inform the
design of effective educational resources, we took a qualitative research approach to
explore young children’s understanding of AI as a technology and a tool with socio-
cultural impact. This approach offered a deeper understanding of how students under-
stand AI, its uses, as well as issues related to ethics. The findings suggest that chil-
dren’s  understanding  of  AI  is  grounded  in  their  personal  experiences  and  habits  as
part of their everyday lives. Furthermore, we found that children have a socio-cultural
approach to AI in which they experience AI as first and foremost a tool that can help,
but also influence their behavior. The goal of this exploratory study was not to draw
generalizable conclusions but rather to construct an in-depth understanding of young
children’s sense-making of AI. Based on our findings, and supporting evidence from
the growing literature on both students’ prior AI conceptions and experiences with AI
ethics curricula, we offer a set of recommendations for the design of engaging and per-
sonally relevant AI education curriculum materials for primary school students.

First, to tackle the common misconception among students that AI is robots, we
recommend  focusing  on  software  applications  of  AI  by  providing  students  with
alternative, more realistic, and above all recognizable applications derived from their
contexts.  Such  an  emphasis  not  only  broadens  their  understanding  of  AI  beyond
robotic portrayals but also grounds the concept in relatable, real-world scenarios.

Secondly, in curriculum design, careful consideration of the socio-cultural con-
text  of  students  is  essential.  Integrating  learning  activities  that  connect  with  com-
mon daily life AI experiences to which students may already have been exposed is
crucial. A practical implementation of this recommendation is to use AI recommen-
dation  software,  such  as  those  found  on  platforms  like  TikTok  or  YouTube,  as  an
accessible entry point for students to grasp what role data has in the working of AI.
By contextualizing AI learning within students’ everyday life experiences, educators
can enhance engagement and foster critical AI literacy.

However, as highlighted by Eguchi et al. (2021) and Druga et al. (2019), prior AI
experiences  are  not  universal  and  can  differ  based  on  several  factors,  such  as  geo-
graphical context, socio-economic status, or family interests. This leads to our third
recommendation:  to  create  AI  experiences  in  the  classroom,  as  a  collective  experi-
ence that can be used as an entry point for other learning activities. For example, by
placing a Hey Google in the classroom and integrating it into the teaching by using
it to set timers during class activities asking it to share a weather report before going
outside, or using a robot vacuum cleaner to clean the classroom. This way, instead
of  assuming  individual  AI  experiences,  the  teacher  can  create  collective  classroom
experiences that can be central to the AI lessons.

Education and Information Technologies

Lastly, our final recommendation proposes to develop  critical AI literacy that places
justice at the forefront and approaches AI as a socio-cultural tool (Avraamidou,  2024).
Although many AI literacy frameworks incorporate an element of AI ethics, the emphasis
often centers on AI as a technology (e.g., Long & Magerko, Touretzky et al., 2019). How-
ever,  both  our  findings  and  existing  literature  illustrate  that  students’  engagement  with
AI is especially intertwined with ethical considerations and socio-cultural implications.
Therefore, adopting a critical AI literacy framework appears to be a promising direction.

Appendix

Semi-structured interview questions:

These questions served as the base for the semi-structured interviews. It’s important
to note that the order of the questions was adjusted based on the flow of the conversation
and  the  responses  of  the  young  children.  In  many  instances,  while  responding  to  one
question, students often addressed other questions, or multiple questions simultaneously.

AI as a technology

–  Can you explain what AI is?
–  How does AI work?
–  What are examples of AI?
–  Where can we find AI?

AI as a tool

–  Do you use AI at home?
–  What do you use AI for?
–  What can AI do?
–  Can AI help us?

AI and ethics

–  Does AI also affect you?
–  Is AI always good for us?
–  Can AI make mistakes?
–  Can AI be dangerous?

During  the  interviews,  the  first  two  groups  eagerly  discussed  an  exercise  at
school  on  AI  biases.  The  students  spoke  about  this  exercise  with  such  emotion
that  the  researcher  decided  to  ask  the  other  groups  about  it  as  well,  to  further
explore the children’s engagement with the topic.

–  Do you remember the exercise where you had to look for ‘baby’ in Google Images?

Education and Information Technologies

Acknowledgements  This project has received funding from the European Union’s Horizon 2020 research
and  innovation  programme  under  grant  agreement  number  101070285  for  the  project  “MAMMOth—
Multi-Attribute, Multimodal Bias Mitigation in AI Systems”. This work reflects only the authors’ views
and the European Research Executive Agency (REA) is not responsible for any use that may be made of
the information it contains.

Data availability  The datasets gathered and analyzed during the current study are available from the cor-
responding author upon reasonable request.

Declarations

Competing interests  The authors declare that they have no competing interests.

Open Access  This  article  is  licensed  under  a  Creative  Commons  Attribution  4.0  International  License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long
as  you  give  appropriate  credit  to  the  original  author(s)  and  the  source,  provide  a  link  to  the  Creative
Commons licence, and indicate if changes were made. The images or other third party material in this
article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line
to the material. If material is not included in the article’s Creative Commons licence and your intended
use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permis-
sion directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/
licenses/by/4.0/.

References

Ali, S., DiPaola, D., Lee, I., Sindato, V., Kim, G., Blumofe, R., & Breazeal, C. (2021). Children as crea-
tors, thinkers and citizens in an AI-driven future. Computers and Education: Artificial Intelligence,
2, 100040–100051. https:// doi. org/ 10. 1016/j. caeai. 2021. 100040

Avraamidou,  L.  (2013).  Superheroes  and  supervillains:  Reconstructing  the  mad-scientist  stereotype  in
school science. Research in Science and Technological Education, 31(1), 90–115. https:// doi. org/ 10.
1080/ 02635 143. 2012. 761605

Heeg,  D.  M.,  Smith,  T.,  &  Avraamidou,  L.  (2022).  Children’s  experiences  and  self-identification  with
science in the context of an out-of-school stem program. EURASIA Journal of Mathematics, Science
and Technology Education, 18(4), Article em2091. https:// doi. org/ 10. 29333/ ejmste/ 11888

Heeg, D. M., & Avraamidou, L. (2023). The use of artificial intelligence in school science: A systematic
literature  review.  Educational  Media  International,  60(2),  125–150.  https:// doi. org/ 10. 1080/ 09523
987. 2023. 22649 90

Avraamidou,  L.  (2024).  Can  we  disrupt  the  momentum  of  the  AI  colonization  of  science  education?
Journal  of  Research  in  Science  Teaching.  Advance  online  publication.  https:// doi. org/ 10. 1002/ tea.
21961

Burgsteiner, H., Kandlhofer, M., & Steinbauer, G. (2016). Irobot: Teaching the basics of artificial intel-
ligence in high schools. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 30,
No. 1). https:// doi. org/ 10. 1609/ aaai. v30i1. 9864

Casal-Otero, L., Catala, A., Fernández-Morante, C., Taboada, M., Cebreiro, B., & Barro, S. (2023). AI
literacy in K-12: A systematic literature review. International Journal of STEM Education, 10(1),
29. https:// doi. org/ 10. 1186/ s40594- 023- 00418-7

Chen, Y., & Tang, X. (2018). Fundamentals of Artificial Intelligence for High Schools. East China Nor-

mal University Press.

DiPaola, D., Payne, B. H., & Breazeal, C. (2020). Decoding design agendas: an ethical design activity for
middle school students. Proceedings of the Interaction Design and Children conference, (pp. 1–10).
https:// doi. org/ 10. 1145/ 33920 63. 33943 96

DiPaola, D., Payne, B. H., & Breazeal, C. (2022). Preparing children to be conscientious consumers and
designers of Ai technologies. In S. Kong & H. Abelson (Eds.), Computational Thinking Education
in K-12: Artificial Intelligence Literacy and Physical Computing. MIT Press.

Education and Information Technologies

Druga,  S.,  Williams,  R.,  Breazeal,  C.,  &  Resnick,  M.  (2017).  Hey  Google  is  it  ok  if  I  eat  you?  Initial
explorations  in  child-agent  interactions.  In  Proceedings  of  the  2017  Conference  on  Interaction
Design and Children, 595–600. ACM. https:// doi. org/ 10. 1145/ 30780 72. 30843 30

Druga, S., Vu, S. T., Likhith, E., & Qiu, T. (2019). Inclusive AI literacy for kids around the world. In P.
Blikstein  &  N.  Holberts  (Eds).,  Proceedings  of  FabLearn  2019  8th  Annual  Conference  on  Maker
Education (pp. 104–111). The Association for Computing Machinery. https:// doi. org/ 10. 1145/ 33118
90. 33119 04

Eguchi, A., Okada, H., & Muto, Y. (2021). Contextualizing AI education for K-12 students to enhance
their  learning  of  AI  literacy  through  culturally  responsive  approaches.  KI-Künstliche  Intelligenz,
35(2), 153–161. https:// doi. org/ 10. 1007/ s13218- 021- 00737-3

Henry, J., Hernalesteen, A., & Collard, A. S. (2021). Teaching artificial intelligence to K-12 through a
role-playing game questioning the intelligence concept. KI-Künstliche Intelligenz, 35(2), 171–179.
https:// doi. org/ 10. 1007/ s13218- 021- 00733-7

Kandlhofer,  M.,  Steinbauer,  G.,  Hirschmugl-Gaisch,  S.,  &  Huber,  P.  (2016).  Artificial  intelligence  and
computer science in education: From kindergarten to university. In 2016 IEEE Frontiers in Educa-
tion Conference (FIE) (pp. 1–9). IEEE. https:// doi. org/ 10. 1109/ FIE. 2016. 77575 70

Kim,  K.,  Kwon,  K.,  Ottenbreit-Leftwich,  A.,  Bae,  H.,  &  Glazewski,  K.  (2023).  Exploring  middle
school  students’  common  naive  conceptions  of  artificial  intelligence  concepts,  and  the  evolu-
tion of these ideas. Education and Information Technologies, (20230118). https:// doi. org/ 10. 1007/
s10639- 023- 11600-3

Kong, S. C., & Zhang, G. (2021). A Conceptual framework for designing artificial intelligence literacy
programmes for educated citizens. In S. C. Kong, Q. Wang, R. Huang, Y. Li, & T. C. Hse (eds).,
Conference  Proceedings  (English  Track)  of  the  25th  Global  Chinese  Conference  on  Computers  in
Education, GCCCE 2021 (pp. 11-15). The Education University of Hong Kong

Lee, I., Ali, S., Zhang, H., DiPaola, D., & Breazeal, C. (2021). Developing middle school students’ AI
literacy. In Proceedings of the 52nd ACM technical symposium on computer science education (pp.
191–197). https:// doi. org/ 10. 1145/ 34088 77. 34325 13

Long,  D.,  &  Magerko,  B.  (2020).  What  is  AI  literacy?  Competencies  and  design  considerations.  In  R.
Bernhaupt, F. F. Muller, D. Verweij & J. Andres (Eds)., Proceedings of the 2020 CHI conference on
human factors in computing systems (pp. 1–16). Association for Computing Machinery. https:// doi.
org/ 10. 1145/ 33138 31. 33767 27

Merriam,  S.  B.,  &  Tisdell,  E.  J.  (2015).  Qualitative  research:  A  guide  to  design  and  implementation.

Wiley.

Mertala,  P.,  Fagerlund,  J.,  &  Calderon,  O.  (2022).  Finnish   5th  and   6th-grade  students’  pre-instructional
conceptions of artificial intelligence (AI) and their implications for AI literacy education. Comput-
ers and Education: Artificial Intelligence, 100095. https:// doi. org/ 10. 1016/j. caeai. 2022/ 100095
Micheuz, P. (2020). Approaches to Artificial Intelligence as a Subject in School Education. In T. Brinda,
D. Passey, & T. Keane (Eds), Empowering Teaching for Digital Equity and Agency. OCCE 2020.
IFIP Advances in Information and Communication Technology, 595. Springer.

Ng,  D.  T.  K.,  Leung,  J.  K.  L.,  Chu,  S.  K.  W.,  &  Qiao,  M.  S.  (2021).  Conceptualizing  AI  literacy  an
exploratory review. Computers and Education: Artificial Intelligence, 2, 100041. https:// doi. org/ 10.
1016/j. caeai. 2021. 100041

Ng, D. T. K., Leung, J. K. L., Su, M. J., Yim, I. H. Y., Qiao, M. S., & Chu, S. K. W. (2022a). AI literacy
for All. AI literacy in K-16 classrooms (pp. 21–30). Springer International Publishing. https:// doi.
org/ 10. 1007/ 978-3- 031- 18880-0_3

Ng, D. T. K., Leung, J. K. L., Su, M. J., Yim, I. H. Y., Qiao, M. S., & Chu, S. K. W. (2022b). The Land-
scape of AI Literacy. AI literacy in K-16 classrooms (pp. 31–62). Springer International Publishing.
https:// doi. org/ 10. 1007/ 978-3- 031- 18880-0_4

OECD.  (2021).  AI  and  the  Future  of  Skills,  Volume  1:  Capabilities  and  Assessments.  Educational

Research and Innovation, OECD Publishing, Paris. https:// doi. org/ 10. 1787/ 5ee71 f34- en

Ottenbreit-Leftwich, A., Glazewski, K., Jeon, M., Jantaraweragul, K., Hmelo-Silver, C. E., Scrinber, A.,
Lee, S., Mott, B., & Lester, J. (2022). Lessons Learned for AI Education with Elementary Students
and Teachers. International Journal of Artificial Intelligence in Education, 1–23. https:// doi. org/ 10.
1007/ s40593- 022- 00304-3

Payne, B. H. (2020). Can my algorithm be my opinion?: An AI + Ethics curriculum for Middle School
Students.  Master’s  thesis,  Massachusetts  Institute  of  Technology,  Media  Lab,  Cambridge,  MA,
USA.

Education and Information Technologies

Pedro, F., Subosa, M., Rivas, A., & Valverde, P. (2019) Artificial intelligence in education: Challenges
and opportunities for sustainable development. Paris: UNESCO . Retrieved January 30, 2023 from
https:// unesd oc. unesco. org/ ark:/ 48223/ pf000 03669 94

Redecker, C. (2017). European framework for the digital competence of educators. Office of the Euro-

pean Union.

Schepman,  A.,  &  Rodway,  P.  (2023).  The  General  Attitudes  towards  Artificial  Intelligence  Scale
(GAAIS): Confirmatory Validation and Associations with Personality, Corporate Distrust, and Gen-
eral Trust. International Journal of Human-Computer Interaction, 39(13), 2724–2741. https:// doi.
org/ 10. 1080/ 10447 318. 2022. 20854 00

Su,  J.,  Ng,  D.  T.  K.,  &  Chu,  S.  K.  W.  (2023a).  Artificial  intelligence  (AI)  literacy  in  early  childhood
education:  The  challenges  and  opportunities.  Computers  and  Education:  Artificial  Intelligence,  4.
https:// doi. org/ 10. 1016/j. caeai. 2023. 100124

Su, J., Guo, K., Chen, X., & Chu, S. K. W. (2023b). Teaching artificial intelligence in K–12 classrooms:
a scoping review. Interactive Learning Environments, 1–20. https:// doi. org/ 10. 1080/ 10494 820. 2023.
22127 06

Szczuka,  J.  M.,  Starthmann,  C.,  Szymczyk,  N.,  Mavrina,  L.  &  Krämer,  N.C.  (2022).  How  do  children
acquire knowledge about voice assistants? A longitudinal field study on children’s knowledge about
how voice assistants store and process data. International Journal of Child-Computer Interaction,
33. https:// doi. org/ 10. 1016/j. ijcci. 2022. 100460

Su, J., & Zhong, Y. (2022). Artificial Intelligence (AI) in early childhood education: Curriculum design
and future directions. Computers and Education: Artificial Intelligence, 3. https:// doi. org/ 10. 1016/j.
caeai. 2022. 100072

Touretzky, D., Gardner-McCune, C., Martin, F., & Seehorn, D. (2019). Envisioning AI for K12: What
should  every  child  know  about  AI?  In  Proceedings  of  the  AAAI  Conference  on  Artificial  Intelli-
gence., 33, 9795–9799. https:// doi. org/ 10. 1609/ aaai. v33i01. 33019 795

UNESCO. (2019). Artificial intelligence for sustainable development programme. Retrieved February 2,

2023, from: https:// en. unesco. org/ sites/ defau lts/ files/ mlw20 19- progr amme. pdf

UNESCO. (2022). K-12 AI curricula A mapping of government-endorsed AI curricula. Retrieved January

30, 2023, from https:// unesd oc. unesco. org/ ark:/ 48223/ pf000 03806 02

UNICEF.  (2021).  Policy  guidance  on  AI  for  children.  Retrieved  February  2,  2023  from:  https:// www.

unicef. org/ globa linsi ght/ media/ 2356/ file

Van  Brummelen,  J.,  Heng,  T.,  &  Tabunshschyk,  V.  (2021).  Teaching  tech  to  talk:  K-12  conversational
artificial intelligence literacy curriculum and development tools. Proceedings of the AAAI Confer-
ence on Artificial Intelligence, 35(17), 15655–15663. https:// doi. org/ 10. 1609/ aaai. v35i17. 17844
Vanover,  C.,  Mihas,  P.,  &  Johnny,  Saldaña  (Eds.).  (2022).  Analyzing  and  interpreting  qualitative

research : after the interview (First). SAGE Publications.

Wang,  B.,  Rau,  P.-L.P.,  &  Yuan.  (2023).  Measuring  user  competence  in  using  artificial  intelligence:
Validity and reliability of artificial intelligence literacy scale. Behaviour & Information Technology,
42(9), 1324–1337. https:// doi. org/ 10. 1080/ 01449 29X. 2022. 20727 68

Williams,  R.,  Ali,  S.,  Devasia,  N.,  Dipaola,  D.,  Hong,  J.,  Kaputsos,  S.  P.,  Jodan,  B.,  &  Breazeal,  C.
(2022). AI ethics curricula for middle school youth: Lessons learned from three project-based cur-
ricula.  International  Journal  of  Artificial  Intelligence  in  Education,  1–59.  https:// doi. org/ 10. 1007/
s40593- 022- 00298-y

Williams, R., Park, H. W., Oh, L., & Breazeal, C. (2019). Popbots: Designing an artificial intelligence
curriculum  for  early  childhood  education.  In  33rd  AAAI  Conference  on  Artificial  Intelligence  (pp.
9829–9736). https:// doi. org/ 10. 1609/ aaai. v33i01. 33019 729

Xiong, Y., Wang, J., & Huang, J. (2018). Textbook Series on Artificial Intelligence for Elementary and

Middle Schools. East China Normal University Press.

Yue, M., Jong, M. S. Y., & Dai, Y. (2022). Pedagogical design of K-12 artificial intelligence education: A

systematic review. Sustainability, 14(23), 15620. https:// doi. org/ 10. 3390/ su142 315620

Zhang, H., Lee, I., Ali, S., DiPaola, D., Cheng, Y., & Breazeal, C. (2022). Integrating ethics and career
futures  with  technical  learning  to  promote  AI  literacy  for  middle  school  students:  An  exploratory
study. International Journal of Artificial Intelligence in Education, 33(2), 290–324. https:// doi. org/
10. 1007/ s40593- 022- 00293-3

Publisher’s Note  Springer Nature remains neutral with regard to jurisdictional claims in published maps
and institutional affiliations.

