{"graph_dict": {"Proshares": [["Issued", "Bito"]], "Bito": [["Hold", "Bitcoin futures contracts"], ["Invests in", "Bitcoin futures"], ["Attracts more trading in", "Bitcoin futures"], ["Has assets under management", "$1.1 billion usd"], ["Belongs to", "Futures-based etf category"], ["Belongs to", "Futures-based etf category"], ["Increases", "Market concentration"], ["Alters", "Trader positions"], ["Alters trading activities of", "Different types of traders"], ["Introduction", "Reduces market illiquidity"]], "Gary gensler": [["Expressed preference for", "Etfs that hold bitcoin futures"]], "Sec": [["Approved", "Spot bitcoin etfs"]], "Makarov and schoar": [["Find that", "Order flow explains about 85% of returns"]], "Investors": [["Feel uncomfortable trading spot bitcoin in unregulated exchanges directly and turn to", "Bitcoin etfs"]], "Bito introduction": [["Transfer the bitcoin demand of retail investors into", "Bitcoin futures"], ["Induces high market concentration", "Of largest four traders"], ["Alters trading interactions", "Asset managers and leveraged funds"], ["Affect", "Bitcoin futures market price changes"]], "Leveraged funds": [["Exhibit highest participation rate on", "Both long and short sides"], ["Take opposite side of transactions", "Asset managers"], ["Transmit information", "Asset managers"]], "Asset managers": [["Increase dramatically in trading position", "Long side"], ["Become major long-side participants", "Against short-side leveraged funds"], ["Increase positions on long side", "Bitcoin futures"], ["Serve as receiver", "Trading spillover"], ["Become", "Information transmitters"], ["Is referred to as", "As"]], "Retail traders": [["Take increased participation in bitcoin futures", "Indirectly through bito"]], "Etfs trading": [["Is associated with", "Improvement in short-run informational efficiency"]], "Glosten et al.": [["Find that", "Etf prices are more likely to follow the underlying stock returns"]], "Etf asset managers": [["Interact with", "Institutional investors"]], "Bitcoin futures": [["Exhibit", "Superior price discovery than cboe's"], ["Have improved liquidity", "After bito introduction"], ["Have relatively low participation", "Large banks"]], "Subject": [["Predicate", "Object"], ["Predicate", "Object"], ["Predicate", "Object"], ["Predicate", "Object"], ["Predicate", "Object"], ["Predicate", "Object"], ["Predicate", "Object"], ["Predicate", "Object"], ["Predicate", "Object"]], "1. bitcoin etf": [["Introduces", "Bitcoin futures"]], "2. tff reports": [["Publishes", "Financial futures markets data"]], "3. asset managers": [["Has", "Open interest positions"], ["Trading role", "Strengthened"]], "Traders": [["Hold open interest positions", "Of"]], "Trading position spillover": [["Is examined in", "Cme bitcoin futures"]], "Tvp-var": [["Is used to examine", "Trading spillovers among traders"], ["Model", "Of"]], "Market liquidity": [["Have", "Natural focus of microstructure research"]], "Bitcoin futures return": [["Relate to", "Net trading position"]], "Market": [["Impact", "Of"]], "Skew": [["Was", "0.37"]], "Hedgers": [["Have negative impact on", "Price of bitcoin futures"], ["Hold", "Underlying asset"]], "Speculators": [["Have positive impact on", "Price of bitcoin futures"], ["Enter market by taking long positions", "Corresponding futures"]], "Cme bitcoin futures": [["Have effect on", "Market quality"]], "Net trading positions": [["Held by", "Type i traders"]], "Dealers": [["Is referred to as", "De"], ["Improves liquidity of", "Bitcoin futures"]], "Basisb": [["Represent", "Futures basis"]], "Market liquidity measurements": [["Include amihud's illquidity and average daily volume", "Cme bitcoin futures"]], "Bito introduction dummy": [["Equals one after introduction of bito etf", "Zero otherwise"]], "Ptpi": [["Refer to", "Asset managers"], ["Refer to", "Dealers"], ["Refer to", "Non-reportable traders"], ["Is percentage trading positions held by", "Type i traders"]], "Huang": [["Methodology used by", "Bitcoin futures"]], "Hasbrouck's pricing error variance": [["Is measured by", "\u03a32pe"]], "Liquidity depth": [["Is estimated by", "Huang and amihud methods"]], "Liquidity resilience": [["Is calculated through", "Covariance of successive daily returns"]], "Hasbrouck": [["Proposes", "Technique"]], "Lamihud": [["Is defined as", "Weekly illiquidity measure"]], "Dbito": [["Equals to one after introduction of", "Bito etf"], ["Equals", "One"]], "1. bito": [["Introduction of", "Cme bitcoin futures"]], "2. dealers": [["Percentage trading position", "Negative coefficient"]], "Ptpas": [["Refer to", "Asset managers"]], "Mepe": [["Equal", "Merpe"]], "Bitcoin": [["Has impact on", "Futures"]], "Etf": [["Has introduction", "On bitcoin futures"]], "Variance ratio": [["Is", "16.21"]], "Investor structure in bitcoin futures": [["Has significantly changed", "After bito introduction"]], "Large banks": [["Play a positive role", "In reducing volatility and enhancing market efficiency"]], "Pricing error": [["Determines how closely actual transaction prices track efficient price", "Variance of pricing error"]], "Yu-lun chen": [["Acknowledges financial support from", "Ministry of science and technology"]], "1. var": [["Can be transformed to", "Vma"]], "2. vma coefficients": [["Can be calculated by", "Forecasting the var system"]], "3. pricing error variance": [["May then be computed as", "Sum of alpha and beta values"]], "Deep learning-based models": [["Shown great promise", "Medical image segmentation"]], "Medsam": [["Designed for bridging", "Generalizability across tasks"], ["Outperforms", "State-of-the-art segmentation foundation model"], ["Was trained on", "Diverse and large-scale medical image-mask pairs dataset"], ["Is", "Promptable segmentation model"], ["Is a", "Deep learning-powered foundation model"], ["Trained on", "Over one million medical image-mask pairs"], ["Surpasses", "Specialist models"], ["Is designed for", "Segmentation of anatomical structures"], ["Has been trained on", "Over one million medical image-mask pairs"], ["Has demonstrated capabilities in", "Segmenting a diverse array of targets and robust generalization abilities"], ["Trained on", "20 a100 gpus"], ["Can reduce annotation time cost", "Human annotation study"]], "Jun ma et al.": [["Conducted a comprehensive evaluation", "86 internal validation tasks"]], "Sam7": [["Is used for", "Medical image segmentation"]], "Computed tomography": [["Is", "Dominant modality"], ["Is format of", "Dicom"]], "Bounding box prompts": [["Provide", "Unambiguous spatial context"]], "Method": [["Outperformed by", "Medsam"]], "Segmentation task": [["Performed better on", "Medsam"]], "Model": [["Struggled with", "Targets of weak boundaries"], ["Has", "Layers"]], "Intensity values": [["Range from", "-2000 to 2000 in ct images"]], "Images": [["Resized to size", "1024 \u00d7 1024 \u00d7 3"]], "1. bi-cubic interpolation": [["Was used for", "Resizing images"]], "2. network architecture": [["Utilized in this study", "Was built on transformer architecture"]], "U-net": [["Used to segment", "Mammography"], ["Outperformed deeplabv3+", "On most tasks"], ["Is known for", "Image classification tasks"], ["Is combined with", "Transformer module"], ["Frequently used for", "Segmentation purpose"], ["Lose", "High-resolution information"], ["Introduced in", "Miccai 2015"]], "Dice loss": [["Is defined by", "Ldice = 1 - 2/n (\u2211i=1n gi si"]], "Wilcoxon signed-rank test": [["Is well-suited for", "Comparing paired samples"]], "Segment anything model": [["Have zero-shot medical image segmentation capabilities", "Sam.md"]], "Nature communications": [["Published article in", "2024"]], "Segment anything": [["Is mentioned in", "Ieee international conference on computer vision"]], "2. natural sciences and engineering research council of canada": [["Supported", "Work"]], "3. authors": [["Appreciate", "Data owners"]], "Young children": [["Have conceptualizations of", "Ai as a technology"], ["Have different experiences with", "Voice assistants like hey google or siri"], ["Understanding of", "Ai"]], "Children": [["Experience and understand ai as", "Supportive tool"], ["Exhibit engagement with", "Ethics of ai"], ["Conceptualize through", "Media representations of ai"], ["Understand", "Ai"], ["Have", "Migration background"], ["Shared laptop with built-in camera", "Interviewer"], ["Have", "Understanding"], ["Attribute capability to", "Platforms"], ["Noticed biases in", "Google images"], ["Expressed concerns about", "Ai bias against people of color"], ["Perceive ai as", "Tool"], ["Exhibit", "High level of engagement with ethics of ai"]], "1. china": [["Has", "Mandatory high school curriculum for ai education"]], "2. world": [["Does not have", "National ai curricula implemented"]], "3. mit": [["Developed", "Collection of ai lessons and tools"]], "Kong": [["Propose", "Framework for ai literacy"], ["Is author of", "Transattunet"]], "Ng et al.": [["Conceptualized", "Ai by building on bloom's taxonomy"]], "Long and magerko": [["Developed", "Framework of ai literature through five competencies"]], "Students": [["Have less accurate conceptualizations of", "Ai"], ["Described ai as preprogrammed", "Making it a human-made artifact"], ["Have ai literacy", "Among young learners"], ["Conceptualize", "Ai"], ["Played role in navigating direction", "Interview"], ["Have interactions with", "Robots"], ["Attribute to", "Ai"]], "Ai": [["Have human-like cognitive processes", "Such as knowing and thinking"], ["Has", "Socio-cultural nature"], ["Is developed by humans", "But can make its own decisions"], ["Is intertwined with", "Ethical considerations"]], "Unesco": [["Endorsed", "14 ai curricula"], ["Published", "Artificial intelligence for sustainable development programme"]], "Zhang et al.": [["Designed", "Daily course"]], "Teacher": [["Has experience in", "Digital literacy including ai"]], "Interviews": [["Were conducted by", "First author"]], "Research question": [["Was basis for developing interview questions", "Literature"]], "Young children's understanding of ai": [["Is about", "Ai"]], "Alice's mother": [["Is bob's", "Alice"]], "Sascha": [["Explain", "Ai"]], "Fred": [["Answer researcher's question", "About ai"]], "Artificial intelligence": [["Is", "A computer or a computer program"]], "Google assistant": [["Can", "Turn on the lights"]], "Calendar notifications": [["May be instances of", "Artificial intelligence"]], "Netflix": [["Use ai to", "Suggest content"]], "Tiktok": [["Track behavior", "Provide tailored content"]], "Voice assistants": [["Used by", "Children"]], "Stella": [["Understand that", "Algorithm"], ["Believes that", "Youtube should share her data"], ["Believes", "Should be more diverse"]], "Youtube": [["Get money from", "User data"], ["Earn money by", "Using algorithm"]], "Rogier": [["Distrusts big tech companies with", "Personal data"]], "Google images": [["Mostly showed caucasian babies", "When searching for 'baby'"]], "Developers": [["Held accountable for", "Ai bias"]], "Dipaola et al.": [["Found nuanced views on ethics of ai when it concerns", "Ai technologies that students aged 11 to 14 are familiar with"]], "Ai literacy": [["Is", "Necessary skill set"]], "Critical ai literacy": [["Places justice at forefront", "And approaches ai as socio-cultural tool"]], "Avraamidou": [["Wrote", "Superheroes and supervillains: reconstructing the mad-scientist stereotype in school science"]], "Conceptualizing": [["Is an exploratory review of", "Ai literacy"]], "Thyroid": [["Located below", "Cartilage structure"]], "Enhanced-transunet": [["Used for", "Thyroid nodule image segmentation"], ["Is proposed", "Approach to tackle issues associated with medical picture segmentation"], ["Uses", "Transformer module"], ["Utilize", "Vgg architecture"]], "Thyroid nodules": [["Affect almost all", "Metabolic processes in body"], ["Pressure on trachea", "Health risk"], ["Have", "Invasiveness"]], "Machine learning approaches": [["Generally used in", "Decision support systems"], ["Used in computer-aided systems", "General"]], "Deep learning methods": [["Provide more detailed", "Learning than machine learning"], ["Evolved from machine learning", "Accurate feature processing"]], "Convolutional neural network models": [["Have constraints on", "Capturing whole context"]], "Unet": [["Can successfully segregate", "Small items"], ["Emerged as", "Promising segmentation method"], ["Have suboptimal predictive performance", "Thyroid nodule segmentation"], ["Have", "Architectural flexibility"]], "Transformer": [["Can collect information about", "Overall environment"], ["Enables", "Model to focus on capturing distant spatial interactions"], ["Function on", "One-dimensional sequence"], ["Sequence length", "Patch size squared"]], "Thyroid disorders": [["Most known", "Nodules"]], "Nodules": [["Benign", "Significant portion"]], "Computer-aided systems": [["Use in health field", "Widespread"]], "Convolutional neural networks": [["Used on image data", "Efficient approach"], ["Used in computer decision support systems", "Effective results"]], "U-net architecture": [["Used for image segmentation", "Renowned for success"]], "Skip connections": [["Enabled in u-net architecture", "Combine semantic and geographical information"], ["Enable u-net to learn", "Low-level texture information and high-level semantic properties"], ["Employs", "Unet"], ["Are essential", "For transunet's success"]], "Scholars": [["Have embarked on", "Integration of u-net and transformer models"]], "Section": [["Devoted to", "Comprehensive analysis"]], "Segmentation techniques": [["Used for", "Medical field"]], "Transformer u-net": [["Proposed by", "Yan et al."]], "Vision transformer": [["Investigated by", "Wang et al."]], "Biomedical image segmentation": [["Proposed by", "Sagar"]], "Trans-norm framework": [["Introduced by", "Azad et al."]], "Global attention": [["Improved by", "Liu et al."]], "Ib-transunet": [["Integrated by", "Li et al."]], "Contextual data": [["Extracted by", "Ib-transunet"]], "Gong et al.": [["Proposed", "Trfe+"]], "Thyroid region prior guided feature enhancement network": [["Is", "Proposed method"]], "Semantic gap": [["Poses challenge to", "Seamless integration of features"]], "Transformers": [["Are used for", "Image processing tasks"], ["Offer promising infrastructure", "For image segmentation"], ["Does not fully leverage", "Potential for segmentation"]], "Multi-head self-attention layer": [["Uses dot-product attention", "To acquire data from various tokens"]], "Mlp": [["Treats each token separately", "And performs modifications"]], "1. transformers": [["Need large amount of data", "Pre-train"]], "2. vision transformers": [["Can perform effectively", "Limited datasets"]], "6. photos": [["Are first divided into separate sections", "Then subjected to learned linear projection"]], "Transunet": [["Uses", "Combination of cnn and transformer's encoding strengths"], ["Adopts", "Hybrid cnn-transformer architecture"]], "Cnn": [["Enables", "Retention of higher-resolution feature maps"]], "Patch embeddings": [["Treat each patch as token", "And embed and process by transformer model"]], "Decoder for segmentation": [["Produces segmentation masks for each patch", "Indicating object or scene segment that patch belongs to"]], "Q": [["Represents", "Set of queries"]], "K": [["Corresponds to", "Key parameter"]], "V": [["Corresponds to", "Value parameter"]], "Picture": [["Has", "C channels"]], "Height and weight": [["Represent", "Spatial resolution"]], "Weights": [["Have", "Input features"]], "Features": [["Have", "Output features"]], "Attention(q": [["K", "V"]], "Multi-head attention": [["Enables", "Model to focus on details"]], "Mhatransformer": [["Has", "Multiple heads"]], "Image": [["Reshaped into", "Series of flattened patches"]], "Patch embedding": [["Guarantees", "Consistent latent vector size"]], "Transformer block": [["Consists of", "Multihead self attention and mlp blocks"]], "Encoded picture representation": [["Indicated by", "Z_l"]], "Layer normalization operator": [["Denoted by", "Ln(\u2219"]], "Upscaled encoded feature": [["Reshaped from", "H\u00d7w/ p^2 \u00d7 d to h/p \u00d7 w/p"]], "Feature map": [["Decreased in channel size using", "1 \u00d7 1 convolution"]], "Downsampled size": [["Is", "Much smaller than original image resolution"]], "Image patches": [["Are converted into", "Series of smoothed patches in transformer encoder block"]], "Patch placement": [["Is a step that trains vectorized patches", "Into d-dimensional layout space"]], "Column vectors": [["Generates patch weights", "Which are then arranged into matrix"]], "Score matrix si": [["Is combined with weight assigned to each patch w", "Using function f(\u22c5"]], "Vit-large/16": [["Is", "Model"]], "Layers": [["Are", "24"]], "Hidden size": [["Is", "1024"]], "Mlp size": [["Is", "4096"]], "Heads": [["Are", "16"]], "Params": [["Have value", "307m"]], "Cup": [["Extracts", "Hidden features"]], "Upsampling operation": [["Is used in", "Decoder module"]], "Vit-l/16": [["Use", "Model"]], "Patch size": [["Require", "Processing power"]], "Resnet architecture": [["Employ", "Cnns"]], "Group normalization": [["Update", "Batch normalization layers"]], "Standardized convolution": [["Replace", "Batch normalization layers"]], "Kolesnikov et al.": [["Find", "Enhanced capacity to transfer learning"]], "Segmentation tokens": [["Go via", "Cascade-based upsampling procedure"]], "Transformer layers": [["Employ", "N successive layers"]], "Dnns": [["Use", "Bilinear interpolation and convolutional neural networks"]], "Imagenet": [["Entails pre-training", "Resnet50"]], "Resnet50": [["And", "Transformer layers"]], "Hyperparameters": [["Play crucial role in determining", "Model's performance"]], "\ud835\udf06\ud835\udc36": [["Set to", "0.25"]], "Parameter \ud835\udf06\ud835\udc37": [["Follows exponential ramp-up technique over", "40 iterations"]], "Dice loss function": [["Is used for optimizing", "Enhanced-transunet model"]], "Intersection over union": [["Is a metric used for evaluating", "Segmentation performance"]], "Area under the receiver operating characteristic curve": [["Is a metric used for evaluating", "Segmentation performance"]], "Hd95": [["Is utilized to evaluate", "Accuracy of segmentation borders"]], "Fcn": [["Utilize", "Vgg architecture"]], "Segnet": [["Utilize", "Vgg architecture"]], "Deeplabv3+": [["Equipped with", "Resnet backbone"]], "Cpfnet": [["Equipped with", "Resnet backbone"]], "Basic unet": [["Inferior to", "Alternative models"]], "Tn3k dataset": [["Used for evaluation", "Enhanced-transunet model"]], "Ddti dataset": [["Used for evaluation", "Cpfnet model"]], "Jaccard score": [["Increased by", "6.20% on tn3k test set"]], "Ground truth photos": [["Used for evaluation", "Segmentation performance"]], "Tn3k": [["Exhibit superior performance", "Unet"]], "Vit-large/16 transformer": [["Combined with resnet50 cnn models", "Produce segmentation results close to ground truth reference images"]], "Proposed modified technique": [["Showcase improved precision", "Outcomes of thyroid nodule segmentation"]], "Thyroid nodule images": [["Have separate labels", "Improve the accuracy of thyroid nodule segmentation"]], "Ultrasound images": [["Have", "Low contrast"]], "Deep learning algorithms": [["Perform better than", "Other approaches"]], "Encoder-decoder structure": [["Cause", "Confusion between thyroid structures and irrelevant structures"]], "Attention block": [["Mitigate", "U-net's weak extraction of small targets and edges"]], "Thyroid image segmentation systems": [["Can be applied to", "Segmentation and modeling of different medical images"]], "Ultrasound image segmentation in the thyroid": [["Lack", "Well-designed dataset and adequate evaluation metrics"]], "Interdisciplinary research": [["Is a powerful tool", "To determine crucial biomarkers"]], "Artificial intelligence-based diagnostics": [["Allows doctors", "To use them in their clinical practice"]], "Score matrix": [["And skip connection", "Improve the overall performance of segmentation"]], "Vit models": [["Provide greater scalability", "Especially when trained on large data sets"]], "Philz": [["Is", "Coffee shop"], ["Founded in", "Berkeley"], ["Founded in", "1982"]], "M.d. zeiler": [["Published paper", "Deconvolutional networks"]], "Z. zhou": [["Published paper", "Unet++"]], "O. oktay": [["Published paper", "Attention u-net"]], "Y. chen": [["Published paper", "Channel-unet"]], "H. sun": [["Published paper", "Aunet"]], "Transattunet": [["Is a model for", "Medical image segmentation"]], "2022": [["Is year of publication of", "Kong et al."]], "Arxiv:2107.05274": [["Is identifier of", "Kong et al."]], "Yan et al.": [["Is author of", "After-unet"]], "After-unet": [["Is a model for", "Medical image segmentation"]], "2021": [["Is year of publication of", "Yan et al."]], "Arxiv:2110.10403": [["Is identifier of", "Yan et al."]], "Maji et al.": [["Is author of", "Attention res-unet"]], "Attention res-unet": [["Is a model for", "Semantic segmentation of brain tumors"]]}}